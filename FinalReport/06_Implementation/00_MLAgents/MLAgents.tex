This section will look at how ML-Agents\footnote{\url{https://github.com/Unity-Technologies/ml-agents}} can be used to create machine learning models for the vehicles and the pedestrians. ML-Agents is an open-source project designed to train agents using reinforcement learning and imitation learning. The purpose of this is to allow users to have multiple entities in the scene without controlling all of them. 

For this project, the hope was to simulate the interaction between the different forms of traffic. However, creating and uploading the simulator onto an external server to train can take a long time. The priority was therefore to train a model for the vehicles. This was done by first training on one vehicle and then looking at collaboration learning.  As will be discussed in chapter~\ref{results}, the vehicles have mixed results and time was spent trying to improve this rather than developing the pedestrians further. The ML training scripts can simply be attached to the pedestrians as shown in figure~\ref{06:fig:MLMap4}.  

\subsection{Network Model}
A variety of different model designs have been attempted throughout this project. This section will look at what decisions were made and why that was the case. More information on the tuning can be found in Chapter~\ref{results}.

\subsubsection{Model inputs}
The first thing to look at is the inputs to the model. The common way when looking online for making autonomous vehicles using ML-Agents is to use checkpoints that span the road. The vehicles would then learn to drive towards the checkpoint and away from the walls. The reason why this would not work for this project is that the rays get blocked by the checkpoint. If there is a pedestrian or a car on the other side the vehicle will not observe them. The solution was instead to guide the vehicles in the direction they should drive. The vehicle is therefore given the angle between its direction and the vector going to the target. This was done by using the calculate angle command in Unity which only returns positive values. For the vehicle to know which way to turn, a simple calculation was made where if the target was on the left the angle would be multiplied by -1. The input angle was then scaled to be between -1 and 1. 

To further increase the performance, the angle was passed through different scaling functions. This can be seen in Figure~\ref{06:anglePlot}. The reason for doing this was to strengthen the input single for smaller angles. This would improve the performance for two reasons. Firstly, as the car cannot turn more than 60 degrees the input angle could be scaled in such a way that for larger angles the input would be close to 1. Passing the input angle through a tanh function strengthens the sensitivity for smaller angles. Therefore, the vehicles use $tanh(2.5x/180)$ where x is the angle. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{06_Implementation/00_MLAgents/Images/anglePlot.png}
    \caption[Input angle functions]{The plot shows different functions applied to the input angle. The aim of using $tanh(2.5x/180)$ is that it decreases the change for larger angles, but increase for smaller.} \label{06:anglePlot}
\end{figure}


For the sensing APIs, the options are either to use raycasts or to use a camera sensor. The advantage of using raycasts is that network does not have to be as complex as for the image processing. Another advantage is that when building the simulator as a server build, enabling the rendering is slightly complicated and decreases the speed of the simulator. The advantage of using the camera sensor is that it can more accurately simulate the real environment as a complex network could distinguish what the camera is capturing.  In Unity, this is not needed as the rays can distinguish what type of object it is detecting. However, in the real world, this is impossible. The next step would be to calculate how many inputs from the raycasts there will be. The 3D rays come with several configurations\footnote{\url{https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Design-Agents.md#raycast-observations}} such as the number of rays per direction, max ray degrees, detectable tags and observation stacking. The total size of the created observations is $(Observation Stacks) * (1 + 2 * Rays Per Direction) * (Num Detectable Tags + 2)$. Observation stacking can be used to give the network a limited short term memory. This is done by repeating observations from previous steps. For example, if the input from a sensor was
\begin{equation}
  \begin{aligned}
   step 1: 0.1\\
step 2: 0.2\\
step 3: 0.3\\
step 4: 0.4
\end{aligned}
\end{equation}
then the stacked input would then look something like this:
\begin{equation}
  \begin{aligned}
step 1: [0.1, 0.0, 0.0]\\
step 2: [0.2, 0.1, 0.0]\\
step 3: [0.3, 0.2, 0.1]\\
step 4: [0.4, 0.3, 0.2]\\
\end{aligned}
\end{equation}

Changing observation stacking made very little difference to the performance of the network, so this value was left at 1.

The next two values to decide was the number of rays per direction and the spread of these rays. Ideally, the agent should use as few as possible whilst at the same time having good coverage of the surrounding area. When using too many inputs, the model was had difficulty figuring out where to drive as the signals from the ray sensors would overpower the one pointing the direction. The best result was found with about 6 rays per direction and an angle of 120 degrees. This would be a total of 13 sensors as one is pointing forward. The angle is wide enough so that if an entity comes from behind the vehicle would know not to turn into it. To compensate for fewer sensors the ML agents use a sphere cast rather than a ray cast. This means that the detection area is much larger than a single point. This will help detect pedestrians as can be seen in Figure~\ref{06:MLMap4}. This limits the probability of the pedestrians ending up being between two rays.  

The last input that was given to the model was speed so that the model would have some understanding of its momentum. Ideally, the vehicle would use this to control how much braking was necessary. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{06_Implementation/00_MLAgents/Images/angle.jpg}
    \caption[Angle visualisation]{The input angle to the model is the angle between the forward vector and the vector going from the vehicle to the target.} \label{06:angleDrawing}
\end{figure}

\subsubsection{Model outputs}
The output of the model matches the vehicle controls. Therefore, The output consists of two values, one for the throttle and breaking and one for turning. The main design decision that had to be made was whether to discrete or continuous output values. Chapter~\ref{results} will look further into this, but basically discrete values work better. This is because the vehicle speed grows with the square root of the input value. Discrete values worked therefore better as they produce strong decisions. 


\subsubsection{Model structure}

% Inputs to the model, angle, speed and sensors. Different sensor styles, stack input, sphare cast. Why not camera. 

% Discussion of what the outputs of the network. Choose discrete.

% Why learning by demonstration had to be used as without it took to long for basic behavior

% Network configurations

% Using GAIL and 

\subsection{Learning Environments}
When training using reinforcement learning it is important to have a good environment. Four different environments were used with increasing complexity. To help improve the robustness of the models, the start position and rotation would vary slightly between runs. 

The first map (Figure~\ref{06:fig:MLMap1}) was designed to help the agents learn to drive forward and turn. Every corner had a checkpoint where they received a reward. The issue with this map was that there was not enough randomness, so the map was straightforward no matter where the agent started. 

The second map (Figure~\ref{06:fig:MLMap2}) was a more complex map where the agents would have to turn both left and right with different amounts as they drove around. When training on this map the checkpoint order would flip every other run. This decreased the vehicle performance on this map but increased the robustness. 

The final training map (Figure~\ref{06:fig:MLMap3}) was designed as an intersection where the vehicles would have to navigate around each other to avoid a collision. Up to four vehicles would spawn in one of the checkpoints positions and then drive to a different one. The map was designed in such a way that vehicles would have to wait for each other if they were both going to the same location. 

\begin{figure}[!htbp] 
\centering
\begin{minipage}[t]{.45\textwidth}
\centering
\begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth, left]{06_Implementation/00_MLAgents/Images/MLMap1.JPG}
        \caption[Training environment map 1]{Simple training environment for the vehicles to drive around.}
        \label{06:fig:MLMap1}
    \end{subfigure}
\end{minipage}
\qquad
\begin{minipage}[t]{.45\textwidth}
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth, right]{06_Implementation/00_MLAgents/Images/MLMap2.JPG}
        \caption[Training environment map 2]{A slightly more complex map where the vehicles would have to do both left and right turns.}
        \label{06:fig:MLMap2}
    \end{subfigure}
\end{minipage}
\end{figure}

\begin{figure}[!htbp] 
\centering
\begin{minipage}[t]{.45\textwidth}
\centering
\begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth, left]{06_Implementation/00_MLAgents/Images/MLMap3.JPG}
        \caption[Training environment map 3]{An intersection map to help the collaboration learning. }
        \label{06:fig:MLMap3}
    \end{subfigure}
\end{minipage}
\qquad
\begin{minipage}[t]{.45\textwidth}
    \centering
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth, right]{06_Implementation/00_MLAgents/Images/MLMap4.JPG}
        \caption[Training pedestrians]{Intersection map where a pedestrian meets a vehicle.}
        \label{06:fig:MLMap4}
    \end{subfigure}
\end{minipage}
\end{figure}


\subsection{Game controller}
\subsection{Learning by Demonstration}


\subsection{Rewards and Collaboration Learning}
The reward system changes as the vehicles improved. 

\subsection{Training}


%continuous vs discrete outputs



%mention attempted checkpoints system.


%\subsection{Collaboration Learning}